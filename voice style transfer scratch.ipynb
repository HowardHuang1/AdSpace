{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy\n",
    "import argparse\n",
    "import torchaudio\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_model = {\n",
    "    \"speechbrain/spkrec-xvect-voxceleb\": 512, \n",
    "    \"speechbrain/spkrec-ecapa-voxceleb\": 192,\n",
    "}\n",
    "\n",
    "def f2embed(wav_file, classifier, size_embed):\n",
    "    signal, fs = torchaudio.load(wav_file)\n",
    "    assert fs == 16000, fs\n",
    "    with torch.no_grad():\n",
    "        embeddings = classifier.encode_batch(signal)\n",
    "        embeddings = F.normalize(embeddings, dim=2)\n",
    "        embeddings = embeddings.squeeze().cpu().numpy()\n",
    "    assert embeddings.shape[0] == size_embed, embeddings.shape[0]\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c44e8676a8e45898bbad5c8d3aa51f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hyperparams.yaml:   0%|          | 0.00/2.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed68359a41845c5b6e336582e81b3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding_model.ckpt:   0%|          | 0.00/16.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe951d5621ec45188b9f8304d1609486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mean_var_norm_emb.ckpt:   0%|          | 0.00/3.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e33e6a3b1d4d2ea22b8ef970ab61c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "classifier.ckpt:   0%|          | 0.00/15.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6284bef6dd7f4e5fb41db1d329f3d390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "label_encoder.txt:   0%|          | 0.00/129k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved embeddings for FEMALE VOICE\n"
     ]
    }
   ],
   "source": [
    "### CREATE XVECTOR\n",
    "\n",
    "# Parameters (normally passed as args)\n",
    "single_wav_path = 'FEMALE VOICE.wav'  # specify the path to your .wav file\n",
    "spkemb_root = '.'  # specify your output directory for speaker embeddings\n",
    "speaker_embed = 'speechbrain/spkrec-xvect-voxceleb'  # specify your model identifier for loading the classifier\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(spkemb_root):\n",
    "    print(f\"Create speaker embedding directory: {spkemb_root}\")\n",
    "    os.mkdir(spkemb_root)\n",
    "\n",
    "# Set up device and load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = EncoderClassifier.from_hparams(source=speaker_embed, run_opts={\"device\": device}, savedir=os.path.join('/tmp', speaker_embed))\n",
    "size_embed = 192  # Assuming size of the embedding, adjust based on your model's output\n",
    "\n",
    "# Function to extract embeddings; assuming you have this function defined as f2embed\n",
    "# Example: f2embed(file_path, classifier, embedding_size)\n",
    "def f2embed(file_path, model, emb_size):\n",
    "    signal = model.load_audio(file_path)\n",
    "    \n",
    "    embeddings = model.encode_batch(signal)\n",
    "\n",
    "    # print(embeddings)\n",
    "\n",
    "    return embeddings.squeeze(0).cpu().detach().numpy()  # Move tensor to CPU and convert to NumPy array\n",
    "\n",
    "# Processing the single WAV file\n",
    "utt_id = os.path.basename(single_wav_path).replace(\".wav\", \"\")\n",
    "utt_emb = f2embed(single_wav_path, classifier, size_embed)\n",
    "# numpy.save(os.path.join(spkemb_root, f\"{utt_id}.npy\"), utt_emb)\n",
    "\n",
    "# print(f\"Processed and saved embeddings for {utt_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_xvector(single_wav_path):\n",
    "\n",
    "    # Parameters (normally passed as args)\n",
    "    # single_wav_path = 'FEMALE VOICE.wav'  # specify the path to your .wav file\n",
    "    # spkemb_root = '.'  # specify your output directory for speaker embeddings\n",
    "    speaker_embed = 'speechbrain/spkrec-xvect-voxceleb'  # specify your model identifier for loading the classifier\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(spkemb_root):\n",
    "        print(f\"Create speaker embedding directory: {spkemb_root}\")\n",
    "        os.mkdir(spkemb_root)\n",
    "\n",
    "    # Set up device and load model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    classifier = EncoderClassifier.from_hparams(source=speaker_embed, run_opts={\"device\": device}, savedir=os.path.join('/tmp', speaker_embed))\n",
    "    size_embed = 192  # Assuming size of the embedding, adjust based on your model's output\n",
    "\n",
    "    # Function to extract embeddings; assuming you have this function defined as f2embed\n",
    "    # Example: f2embed(file_path, classifier, embedding_size)\n",
    "    def f2embed(file_path, model, emb_size):\n",
    "        signal = model.load_audio(file_path)\n",
    "\n",
    "        embeddings = model.encode_batch(signal)\n",
    "\n",
    "        # print(embeddings)\n",
    "\n",
    "        return embeddings.squeeze(0).cpu().detach().numpy()  # Move tensor to CPU and convert to NumPy array\n",
    "\n",
    "    # Processing the single WAV file\n",
    "    utt_id = os.path.basename(single_wav_path).replace(\".wav\", \"\")\n",
    "    utt_emb = f2embed(single_wav_path, classifier, size_embed)\n",
    "    \n",
    "    return utt_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD MODEL\n",
    "from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_voice_conversion_model():\n",
    "    dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "    dataset = dataset.sort(\"id\")\n",
    "    sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "    # example_speech = dataset[0][\"audio\"][\"array\"] # this is the speaker 1 input!!!\n",
    "\n",
    "    processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_vc\")\n",
    "    model = SpeechT5ForSpeechToSpeech.from_pretrained(\"microsoft/speecht5_vc\")\n",
    "    vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "    # inputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD XVECTOR\n",
    "import soundfile as sf\n",
    "\n",
    "def execute_voice_conversion(model, source_array, target_speaker_embeddings, output_name):\n",
    "    speaker_embeddings = torch.tensor(target_speaker_embeddings)\n",
    "\n",
    "    speech = model.generate_speech(source_array[\"input_values\"], speaker_embeddings, vocoder=vocoder)\n",
    "\n",
    "    sf.write(output_name, speech.numpy(), samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SpeechT5ForSpeechToSpeech were not initialized from the model checkpoint at microsoft/speecht5_vc and are newly initialized: ['speecht5.encoder.prenet.pos_sinusoidal_embed.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "execute_voice_conversion(load_voice_conversion_model(), utt_emb, \"success.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
       "       0.0010376 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio array: [ 1.2149215e-24 -6.7208425e-25  2.8434334e-24 ... -9.6224318e-04\n",
      " -1.0187828e-03 -1.2711850e-03]\n",
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Path to your WAV file\n",
    "file_path = 'SOURCERECORDING.mp3'\n",
    "\n",
    "# Load the audio file\n",
    "audio, sr = librosa.load(file_path, sr=16000)  # 'sr=None' loads the file with its original sampling rate\n",
    "\n",
    "# audio is the numpy array representing the audio signal\n",
    "# sr is the sampling rate of the audio\n",
    "\n",
    "print(\"Audio array:\", audio)\n",
    "print(\"Sampling rate:\", sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_INPUT = processor(audio=audio, sampling_rate=16000, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_embeddings = torch.tensor(utt_emb)\n",
    "\n",
    "speech = model.generate_speech(NEW_INPUT[\"input_values\"], speaker_embeddings, vocoder=vocoder)\n",
    "output_name = \"TEST ME1.wav\"\n",
    "sf.write(output_name, speech.numpy(), samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_source_audio_array(file_path):\n",
    "    \n",
    "    \n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(file_path, sr=16000)  # 'sr=None' loads the file with its original sampling rate\n",
    "\n",
    "    # audio is the numpy array representing the audio signal\n",
    "    # sr is the sampling rate of the audio\n",
    "\n",
    "    print(\"Audio array:\", audio)\n",
    "    print(\"Sampling rate:\", sr)\n",
    "    \n",
    "    NEW_INPUT = processor(audio=audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    \n",
    "    return NEW_INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SpeechT5ForSpeechToSpeech were not initialized from the model checkpoint at microsoft/speecht5_vc and are newly initialized: ['speecht5.encoder.prenet.pos_sinusoidal_embed.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio array: [ 1.2149215e-24 -6.7208425e-25  2.8434334e-24 ... -9.6224318e-04\n",
      " -1.0187828e-03 -1.2711850e-03]\n",
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "execute_voice_conversion(load_voice_conversion_model(), generate_source_audio_array(\"SOURCERECORDING.mp3\"), \n",
    "                        create_target_xvector(\"FEMALE VOICE.wav\"), \"YEEEE.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio array: [ 1.2149215e-24 -6.7208425e-25  2.8434334e-24 ... -9.6224318e-04\n",
      " -1.0187828e-03 -1.2711850e-03]\n",
      "Sampling rate: 16000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2149e-24, -6.7208e-25,  2.8434e-24,  ..., -9.6224e-04,\n",
       "         -1.0188e-03, -1.2712e-03]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_source_audio_array(\"SOURCERECORDING.mp3\")['input_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
